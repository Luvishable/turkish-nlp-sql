# src/nlp/berturk_test.py
from transformers import AutoTokenizer, AutoModel
import torch
import time


def test_berturk_loading():
    """BERTurk model yÃ¼kleme ve temel test"""
    print("ğŸ¤– BERTurk model yÃ¼kleniyor...")

    try:
        start_time = time.time()

        # Model ve tokenizer yÃ¼kle
        tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased")
        model = AutoModel.from_pretrained("dbmdz/bert-base-turkish-cased")

        load_time = time.time() - start_time
        print(f"âœ… Model yÃ¼klendi: {load_time:.2f} saniye")

        # Test prompt'larÄ±
        test_prompts = [
            "mÃ¼ÅŸteri listesini gÃ¶ster",
            "Ã¼rÃ¼n sayÄ±sÄ±nÄ± hesapla",
            "bu ayÄ±n sipariÅŸlerini listele",
            "satÄ±ÅŸ toplamÄ±nÄ± bul"
        ]

        print("\nğŸ” Test prompt'larÄ±:")
        for prompt in test_prompts:
            process_start = time.time()

            # Tokenization
            tokens = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)

            # Model inference
            with torch.no_grad():
                outputs = model(**tokens)

            process_time = (time.time() - process_start) * 1000

            print(f"  ğŸ“ '{prompt}'")
            print(f"     Tokens: {tokens['input_ids'].shape[1]} adet")
            print(f"     Ä°ÅŸlem sÃ¼resi: {process_time:.1f}ms")
            print(f"     Embedding shape: {outputs.last_hidden_state.shape}")
            print()

        print("ğŸ‰ BERTurk test baÅŸarÄ±lÄ±!")
        return True

    except Exception as e:
        print(f"âŒ BERTurk test baÅŸarÄ±sÄ±z: {e}")
        return False


def test_turkish_tokenization():
    """TÃ¼rkÃ§e tokenization testi"""
    print("ğŸ‡¹ğŸ‡· TÃ¼rkÃ§e tokenization testi...")

    try:
        tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased")

        # TÃ¼rkÃ§e karakterler ve ek'ler
        turkish_tests = [
            "mÃ¼ÅŸteri",
            "mÃ¼ÅŸteriler",
            "mÃ¼ÅŸterilerin",
            "mÃ¼ÅŸterilerimizin",
            "gÃ¶ster",
            "gÃ¶sterir",
            "gÃ¶stermek"
        ]

        for text in turkish_tests:
            tokens = tokenizer.tokenize(text)
            print(f"  '{text}' â†’ {tokens}")

        print("âœ… TÃ¼rkÃ§e tokenization Ã§alÄ±ÅŸÄ±yor!")
        return True

    except Exception as e:
        print(f"âŒ Tokenization test baÅŸarÄ±sÄ±z: {e}")
        return False


if __name__ == "__main__":
    print("=" * 50)
    print("ğŸš€ BERTurk Integration Test")
    print("=" * 50)

    # Model loading test
    loading_success = test_berturk_loading()

    print("\n" + "=" * 30)

    # Turkish tokenization test
    tokenization_success = test_turkish_tokenization()

    print("\n" + "=" * 30)
    print("ğŸ“Š Test Ã–zeti:")
    print(f"  Model Loading: {'âœ…' if loading_success else 'âŒ'}")
    print(f"  Tokenization: {'âœ…' if tokenization_success else 'âŒ'}")

    if loading_success and tokenization_success:
        print("\nğŸ‰ TÃ¼m testler baÅŸarÄ±lÄ±! BERTurk hazÄ±r!")
    else:
        print("\nâŒ BazÄ± testler baÅŸarÄ±sÄ±z!")